{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# FILE = Path(__file__).absolute()\n",
    "# sys.path.append(FILE.parents[0].as_posix())\n",
    "\n",
    "from yolobot_recognition.models.common import DetectMultiBackend\n",
    "from yolobot_recognition.utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from yolobot_recognition.utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from yolobot_recognition.utils.plots import Annotator, colors, save_one_box\n",
    "from yolobot_recognition.utils.torch_utils import select_device, smart_inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-6-10 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2060, 5919MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "weights = 'yolov5s.pt'  # model.pt path(s)\n",
    "imgsz = (640, 480)  # inference size (pixels)\n",
    "conf_thres = 0.25  # confidence threshold\n",
    "iou_thres = 0.45  # NMS IOU threshold\n",
    "max_det = 1000  # maximum detections per image\n",
    "device_num = ''  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "dnn = False\n",
    "data = 'data/coco128.yaml'  # dataset.yaml path\n",
    "line_thickness = 3  # bounding box thickness (pixels)\n",
    "hide_labels = False  # hide labels\n",
    "hide_conf = False  # hide confidences\n",
    "image_path = 'images/test_humans_up.png'  # path to your image\n",
    "\n",
    "# Initialize\n",
    "device = select_device(device_num)\n",
    "\n",
    "# Load model\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=False)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "# Load image\n",
    "img0 = cv2.imread(image_path)  # BGR\n",
    "assert img0 is not None, 'Image not found ' + image_path\n",
    "\n",
    "# Padded resize\n",
    "img = cv2.resize(img0, (imgsz[0], imgsz[1]))\n",
    "\n",
    "# Convert BGR to RGB\n",
    "img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "img = np.ascontiguousarray(img)\n",
    "\n",
    "# Run inference\n",
    "model.warmup(imgsz=(1, 3, *imgsz))  # warmup\n",
    "img = torch.from_numpy(img).to(device)\n",
    "img = img.float()  # uint8 to fp16/32\n",
    "img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "if img.ndimension() == 3:\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "# Inference\n",
    "pred = model(img, augment=False, visualize=False)\n",
    "\n",
    "# Apply NMS\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, None, False, max_det=max_det)\n",
    "\n",
    "# Process detections\n",
    "for i, det in enumerate(pred):  # detections per image\n",
    "    s = f'{i}: '\n",
    "    s += '%gx%g ' % img.shape[2:]  # print string\n",
    "\n",
    "    annotator = Annotator(img0, line_width=line_thickness, example=str(names))\n",
    "    if len(det):\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "        # Print results\n",
    "        for c in det[:, -1].unique():\n",
    "            n = (det[:, -1] == c).sum()  # detections per class\n",
    "            s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "        \n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            c = int(cls)  # integer class\n",
    "            label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "            annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"IMAGE\", img0)\n",
    "cv2.waitKey(0)  # Press any key to close the window\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample set with new values\n",
    "new_values = set()\n",
    "\n",
    "new_values.add(2)\n",
    "new_values.add(2)\n",
    "new_values.add(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
